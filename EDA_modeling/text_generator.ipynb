{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Importing libraries"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.utils import np_utils\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Due to the size of the data, opening csv file as an iterator and extracting 10,000 rows."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "raw_data = pd.read_csv('data.csv', iterator=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "text_gen_data = raw_data.get_chunk(100000)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using the category funny to train model on funny reviews."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "source": [
    "text_gen_data = text_gen_data.loc[text_gen_data.label == 'funny']\n",
    "text_gen_data"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>So my roommate borrowed my clock radio and app...</td>\n",
       "      <td>funny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>Was not that great. We sat for like 15 mins be...</td>\n",
       "      <td>funny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>We received terrible service when we ordered a...</td>\n",
       "      <td>funny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>I usually go into places like chilis with pret...</td>\n",
       "      <td>funny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>\"I overheard you are out of the roast beef, ri...</td>\n",
       "      <td>funny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99742</th>\n",
       "      <td>After shopping on Newbury Street for a good wh...</td>\n",
       "      <td>funny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99765</th>\n",
       "      <td>You should never bring a girl here on a date.\\...</td>\n",
       "      <td>funny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99825</th>\n",
       "      <td>DON'T GET THE BRAZILIAN!!!\\nI went there last ...</td>\n",
       "      <td>funny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99826</th>\n",
       "      <td>I had an appointment and arrived on time for i...</td>\n",
       "      <td>funny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99857</th>\n",
       "      <td>Don't try to trick me into believing that sirl...</td>\n",
       "      <td>funny</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1106 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "66     So my roommate borrowed my clock radio and app...  funny\n",
       "69     Was not that great. We sat for like 15 mins be...  funny\n",
       "305    We received terrible service when we ordered a...  funny\n",
       "309    I usually go into places like chilis with pret...  funny\n",
       "375    \"I overheard you are out of the roast beef, ri...  funny\n",
       "...                                                  ...    ...\n",
       "99742  After shopping on Newbury Street for a good wh...  funny\n",
       "99765  You should never bring a girl here on a date.\\...  funny\n",
       "99825  DON'T GET THE BRAZILIAN!!!\\nI went there last ...  funny\n",
       "99826  I had an appointment and arrived on time for i...  funny\n",
       "99857  Don't try to trick me into believing that sirl...  funny\n",
       "\n",
       "[1106 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 63
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Cleaning text by removing punctuation and stopwords."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "clean_punct_text = []\n",
    "for review in text_gen_data.text:\n",
    "    review = review.lower()\n",
    "    \n",
    "    review = review.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    clean_punct_text.append(review)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "stopwords = nlp.Defaults.stop_words\n",
    "\n",
    "clean_no_stopwords = []\n",
    "for review in clean_punct_text:\n",
    "    review = set(review.split())\n",
    "    clean_review = review.difference(stopwords)\n",
    "    clean_no_stopwords.append(' '.join(clean_review))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Extracting unique words to assing them a number and create a dictionary."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "source": [
    "text_gen_data_all = []\n",
    "\n",
    "for review in clean_no_stopwords:\n",
    "    text_gen_data_all.extend(review.split())\n",
    "\n",
    "unique_words = list(set(text_gen_data_all))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "source": [
    "num_to_word = {k:v for k,v in enumerate(unique_words)}\n",
    "word_to_num = {v:k for k,v in enumerate(unique_words)}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Creating dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "X_text_gen = []\n",
    "y_text_gen = []\n",
    "\n",
    "for review in clean_no_stopwords:\n",
    "    \n",
    "    split_review = review.split()\n",
    "    \n",
    "    for i in range(len(split_review)):\n",
    "        try:\n",
    "            x = split_review[i:i+3]\n",
    "            y = split_review[i+4]\n",
    "            \n",
    "            X_text_gen.append(x)\n",
    "            y_text_gen.append(y)\n",
    "        except:\n",
    "            break"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Turning words to assigned number"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "X_text_gen_processed = []\n",
    "\n",
    "for i in X_text_gen:\n",
    "    temp_list = []\n",
    "    \n",
    "    for word in i:\n",
    "        temp_list.append(word_to_num.get(word))\n",
    "    \n",
    "    X_text_gen_processed.append(temp_list)\n",
    "\n",
    "y_text_gen_processed = []\n",
    "\n",
    "for word in y_text_gen:\n",
    "    y_text_gen_processed.append(word_to_num.get(word))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Processing input arrays and encoding target variable"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "y = np_utils.to_categorical(y_text_gen_processed)\n",
    "X = np.array(X_text_gen_processed)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "X = np.reshape(X,(X.shape[0],X.shape[1],1))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Building and fitting model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(400, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(400, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(400))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.fit(X, y, epochs=100, batch_size=50)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Due to model performance requirements google colab was used to train the model. The following is importing the trained model and testing the output."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "import keras"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "model = keras.models.load_model('word_model.h5')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "review = []\n",
    "start = np.random.randint(0,14685,3)\n",
    "review.extend(start)\n",
    "for i in range(100):\n",
    "    last_three = review[i:i+3]\n",
    "    last_three = np.array(last_three).reshape((1,3,1))\n",
    "    preds = np.argmax(model.predict(last_three))\n",
    "    review.append(preds)\n",
    "    \n",
    "review_word = []\n",
    "for i in review:\n",
    "    review_word.append(num_to_word.get(i))\n",
    "' '.join(review_word)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'traveling mexican nuance coladas cutest appraised appraised coladas steps appraised coladas steps appraised coladas steps appraised coladas steps appraised coladas steps appraised coladas steps appraised coladas steps appraised coladas steps appraised coladas steps appraised coladas steps appraised coladas steps appraised coladas steps appraised coladas steps appraised coladas steps appraised coladas steps appraised coladas steps appraised coladas steps appraised coladas steps appraised coladas steps appraised coladas steps appraised coladas steps appraised coladas steps appraised coladas steps appraised coladas steps appraised coladas steps appraised coladas steps appraised coladas steps appraised coladas steps appraised coladas steps appraised coladas steps appraised coladas steps appraised coladas steps appraised'"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Iteration 2"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this iteration, the same steps as before were taken. The difference is that this model deals with character level predictions rather than word label."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "source": [
    "char_raw_data =raw_data.get_chunk(10000)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "source": [
    "clean_punct_text = []\n",
    "for review in char_raw_data.text:\n",
    "    if review is not np.nan:\n",
    "        lower_review = review.lower()\n",
    "        no_punct = lower_review.translate(str.maketrans('', '', string.punctuation))\n",
    "        s = re.sub(r'[^a-z]+ ', '', no_punct)\n",
    "        clean_punct_text.append(s)\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "source": [
    "stopwords = nlp.Defaults.stop_words\n",
    "\n",
    "clean_no_stopwords = []\n",
    "for review in clean_punct_text:\n",
    "    review = set(review.split())\n",
    "    clean_review = review.difference(stopwords)\n",
    "    clean_no_stopwords.append(' '.join(clean_review))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "source": [
    "chars = [char for review in clean_no_stopwords for char in review if char.isascii()]\n",
    "chars_unique = set(chars)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "source": [
    "lower_case_letters = set(list(string.ascii_lowercase))\n",
    "lower_case_letters.add(' ')\n",
    "split_reviews = []\n",
    "for review in clean_no_stopwords:\n",
    "    review_split = list(review)\n",
    "    temp_list = []\n",
    "    for letter in review_split:\n",
    "        if letter in lower_case_letters:\n",
    "            temp_list.append(letter)\n",
    "    split_reviews.append(''.join(temp_list))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "source": [
    "x_char_gen = []\n",
    "y_char_gen = []\n",
    "\n",
    "for review in clean_no_stopwords:\n",
    "    \n",
    "    split_review = list(review)\n",
    "    for i in range(len(split_review)):\n",
    "        \n",
    "        try:\n",
    "            x = split_review[i:i+3]\n",
    "            y = split_review[i+4]\n",
    "            \n",
    "            x_char_gen.append(x)\n",
    "            y_char_gen.append(y)\n",
    "        except:\n",
    "            break"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "char_to_num = {v:k for k,v in enumerate(list(string.ascii_lowercase + ' '))}\n",
    "num_to_char = {k:v for k,v in enumerate(list(string.ascii_lowercase + ' '))}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "source": [
    "X_char_gen_processed = []\n",
    "\n",
    "for i in x_char_gen:\n",
    "    temp_list = []\n",
    "    \n",
    "    for char in i:\n",
    "        char_num = char_to_num.get(char,0)\n",
    "        temp_list.append(char_num)\n",
    "    X_char_gen_processed.append(temp_list)\n",
    "\n",
    "y_char_gen_processed = []\n",
    "\n",
    "for word in y_char_gen:\n",
    "    char_num = char_to_num.get(char,0)\n",
    "    y_char_gen_processed.append(char_num)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "source": [
    "np.save('x.npy', X_char_gen_processed)\n",
    "np.save('y.npy', y_char_gen_processed)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "y = np_utils.to_categorical(y_char_gen_processed)\n",
    "X = np.array(X_char_gen_processed)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "X = np.reshape(X,(X.shape[0],X.shape[1],1))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.fit(X, y, epochs=2, batch_size=500)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "model = keras.models.load_model('/Users/alejandro/Documents/Coding/ML/yelp_reviews/data/char_model.h5')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "print(char_to_num.get('l'))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "11\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "review = []\n",
    "start = np.random.randint(0,26,3)\n",
    "review.extend(start)\n",
    "for i in range(100):\n",
    "    last_three = review[i:i+3]\n",
    "    last_three = np.array(last_three).reshape((1,3,1))\n",
    "    preds = np.argmax(model.predict(last_three))\n",
    "    review.append(preds)\n",
    "    \n",
    "review_char = []\n",
    "for i in review:\n",
    "    review_char.append(num_to_char.get(i))\n",
    "' '.join(review_char)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'e h g i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i'"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.7 64-bit"
  },
  "interpreter": {
   "hash": "7e85ec9bf098c5427e45e2f632dcd4eeff803b007e1abd287d600879388709c1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}